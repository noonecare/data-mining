# 特征选择与稀疏学习



### Algorithms

#### Relief

$$\delta^j = \sum_{i} -diff(x_i^j, x_{i, nh}^j)^2 + diff(x_i^j, x_{i, nm}^j)$$

$x_{i, nh}$ near-hit 同类中的最近邻

$x_{i, nm}$ near-miss 异类中的最近邻

因为需要计算所有点之间的距离，还要排序，所以我感觉这个算法效率应该不是很高。
除此之外，diff 需要自定义，大体相当于距离。



### Question

11.1 编程实现 Relief 算法，并考察其在西瓜数据集 3.0 上的运行结果。

11.2 写出 Relief-F 的算法描述

11.3 原来是 x_1, x_2, ..., x_n 变成 (x_i, x_j) n * (n - 1) / 2 个 feature， 然后套用 Relief 算法即可。

11.4 改进 LVW, 使得即便有运行时间限制，该算法也能给出解。

11.5 如果解出现在与横轴坐标没有交点的平方误差项等值线上时，L1 正则化不能产生稀疏解。

11.6 岭回归：

$$min_{\vec{w}} \sum_{i=1}^{m}(y_i - \vec{w}^T \vec{x}_i)^2 + \lambda ||\vec{w}||_1$$
当把 $L_1$ 范数的正则项替换成 $L_2$ 范数的正则项时，岭回归等同于SVM。
11.7 $L_0$ $L_0$ 范数没有等值线，因为在一个象限中， $L_0$ 范数是个定值。这就使得 L_0 范数的限制太强，很容易找到解。
11.8 西瓜书已经给出证明，不知还要证明什么。
11.9 字典学习和压缩感知对稀疏性利用的异同：字典学习得到的样本本身是稀疏的，只是表示方法不是系数的。字典学习学习字典，使得可以把样本表示成稀疏的。压缩感知是已知样本是系数的，但是样本中某些特征值不全，根据这些不全的样本，还原出全的样本。
11.10 