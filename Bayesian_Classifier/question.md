# 贝叶斯分类器


### Intuition（主观意图）

如果有先验概率分布，那么我们对于模型最好的估计就是能使得后验概率极大的模型。


### Hypothesis and Cost Function（预测函数和损失函数）

#### 贝叶斯决策论

一般是个参数估计的问题，首先假设样本的分布符合某种分布，比如属于高斯分布。

后验概率 $P(c|\vec{x})$ 为：

$$P(c|\vec{x}) = \frac {P(\vec{x}|c) P(c)}{P(\vec{x})}$$


$P(c)$ 的概率就是 class 为 c 的样本数占总样本数的比率。$P(\vec{x})$ 与 $\vec{x}$ 属于哪个 class 无关。主要就是要想办法估计 $P(\vec{x} | c)$。


最简单地如果样本量足够大，而 $\vec{x}$ 的可取的值比较少（为自然数），那么同样计算一下频率，就可以估计出 $P(\vec{x}|c)$。不过很多情况下 $\vec{x}$ 中存在取值为连续值的 feature，这时候，你没法这样做。你需要做先验假设，然后估计 $P(\vec{x}|c)$。



#### 朴素贝叶斯分类器（Naive Bayes Classifier）

朴素贝叶斯给出的先验假设为 $\vec{x}$ 的所有 feature 相互独立，而且属于同一簇分布 $P(\theta, c)$（比如都是正态分布，但是正态分布的 $\mu$, $\sigma$ 不必一致）。有了用参数 $\theta$ 表示的后验概率，可以根据极大似然估计给出参数的值，进而给出任何一个样本点属于任意一个 class 的概率，取概率最大的 class 为样本点预测的 class 。

#### 极大似然估计

$$P(D_c | \theta_c) = \prod_{\vec{x} \in D_{c}} P(\vec{x}|\theta_{c})$$

选择使得上式最大的 $\theta$



#### 半朴素贝叶斯分类器

半朴素贝叶斯分类器，不再假设样本点的各个 feature 是独立的。

SPODE 独依赖假设（然后所有 feature 都只依赖于某一个 feature）

TAN	没有看懂最大带权生成树是什么意思

AODE 

#### 贝叶斯网

- $$B = < G, \Theta >$$
- 条件概率表（ Conditional Probability Table）,就是 child 属性相对于 parent 属性的条件概率 $\theta_{x_i | \pi_{i}} = P_{B}(x_i|\pi_i)$（这个值就是表示贝叶斯网中每条边的权重）
- 结构(G)， 评分搜索可以计算出 G, 评分搜索需要评分函数， 常用的评分函数如下
$$s(B|D) = f(\theta) |B| - \sum_{i=1}^{m} log P_{B}(\vec{x_i})$$

上面的公式中 $f(\theta)$ 表示编码 B 模型每个参数需要多少字节。上面的公式前一部分表示表示模型需要的字节数，后一部分表示编码（可以认为是用霍夫曼编码）所有样本需要的字节数。这个评分函数 $s(B|D)$ 表示的是编码模型和样本点总共需要的字节数。
特别地，当$f(\theta)=1$ 时，这个评分函数表示的是 AIC, 当 $f(\theta)= \frac {logm} {2}$ 时，表示的是 BIC。从这里看出信息学中这两个判据共同的特点是要求用尽可能少的字节编码模型和样本点。找出 G 是个 NP 难的问题，比如我给个找法，穷举所有可能网络结构，找出能使评分函数最小的网络结构。简单一句话，找网络结构暂时没有什么好办法。条件概率表可以直接计算出来，所以找到 G ， 也就完成了贝叶斯网。

- 推断（预测）
贝叶斯网表示的属性之间的依赖关系，要想计算出 $P(c|\vec{x})$ 还是一个 NP 难的问题。复杂度太高所以一般用 Gibbs Sampling 给出近似答案。
（没有搞清楚 Gibbs Sampling 的原理）


#### EM 算法

EM 其实是对于包含隐变量的模型的参数估计的方法。

- E 步
$$Q(\Theta | \Theta^t) = E_{\vec{Z}|\vec{X}, \Theta^t}LL(\Theta | \vec{X}, \vec{Z})$$

- M 步

$$\Theta^{t+1}= arg max_{\Theta} Q(\Theta | \Theta^t)$$

$\Theta^0$ 任意取值，经过足够多次的迭代， $\Theta^t$ 收敛到 $\Theta$

### 模型的优势和缺陷

- 自然而然就是个多分类的模型。
- 需要有先验概率分布，这个选择很主观。一般有社科多幂率，自然科学多正态。
- 在计算方面，可能出现下溢，有个 feature 训练出来的概率为0,导致其他 feature 的影响全被抹去了。


### Implementation（实现）



### question

7.1 试试用极大似然法估算西瓜数据集3.0中前三个属性的类条件概率。

7.2




