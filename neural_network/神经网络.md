# 神经网络


### Intuition

可能是对于人神经系统的研究，引出了神经网络模型。神经网络中的模型有很多概念的名字提起来很像神经系统的概念。


- 神经元
    - 输入的加权评论
    - 激励函数

### Hypothesis and Cost Function

神经网络的表达能力非常强（只要神经元足够多，可以趋近任何连续函数）。




### Qustions

    5.1 试述将线性函数 $f(x) = \vec{w}^{T} \vec{x}$ 用作神经元激励函数的缺陷。
        如果用线性函数作为激励函数，那么最后预测函数也是个线性函数，神经网络也就不再具有表达复杂非线性的能力，比如对于线性不可分问题，将无法给出好的分类模型。
    
    5.2 试述使用图 5.2(b) 激活函数的神经元与对率回归的联系。
        每个神经元都是一个对率回归模型，神经网络模型成为对率回归函数的复合。
        
    5.3 对于图 5.7 中的 $v_{ih}$, 试推导出 BP 算法中的更新公式 (5.13).
    
$$\alpha_{h} = \sum_{i=1}^{d} {v_{ih} x_i} \Rightarrow \frac {\partial \alpha_{h}}{\partial {v_{ih}}} = x_i$$

$$\frac{\partial {b_h}}{\partial {v_{ih}}} = \frac {\partial {b_h}}{\partial {\alpha_{h}}} \frac {\partial \alpha_{h}}{\partial {v_{ih}}} \land \frac {\partial \alpha_{h}}{\partial {v_{ih}}} = x_i  \Rightarrow \frac{\partial {b_h}}{\partial {v_{ih}}} = \frac {\partial {b_h}}{\partial {\alpha_{h}}} x_i$$

$$\frac {\partial {E_{k}}} {\partial {v_{ih}}} = \frac {\partial {E_{k}}}{\partial {b_{h}}} \frac{\partial {b_h}}{\partial {v_{ih}}} \land \frac{\partial {b_h}}{\partial {v_{ih}}} = \frac {\partial {b_h}}{\partial {\alpha_{h}}} x_i \Rightarrow \frac {\partial {E_{k}}} {\partial {v_{ih}}} = \frac {\partial {E_{k}}}{\partial {b_{h}}} \frac {\partial {b_h}}{\partial {\alpha_{h}}} x_i$$

$$\frac {\partial {E_{k}}} {\partial {v_{ih}}} = \frac {\partial {E_{k}}}{\partial {b_{h}}} \frac {\partial {b_h}}{\partial {\alpha_{h}}} x_i \land e_h = \frac {\partial {E_{k}}}{\partial {b_{h}}} \frac {\partial {b_h}}{\partial {\alpha_{h}}} \Rightarrow \frac {\partial {E_{k}}} {\partial {v_{ih}}} = e_h x_i$$

$$\Delta v_{ih} = -\eta \frac {\partial {E_{k}}} {\partial {v_{ih}}} \land \frac {\partial {E_{k}}} {\partial {v_{ih}}} = e_h x_i \Rightarrow \Delta v_{ih} = - \eta e_h x_i$$
        
    5.4 试述式（5.6）中学习率的取值对于神经网络的训练的影响。
        学习率小，收敛太慢。学习率大，结果可能会震荡不收敛。
        
    5.5 试编程实现标准 BP 算法和累积 BP 算法，在西瓜数据集 3.0 上分别用这两个算法训练一个单隐层网络，并进行比较。
    	标准BP 和 累计 BP 算法的不同在于： 标准 BP 每次更新权重和阈值，是以某一个样本的 $$E_k$$ 为 cost function 计算的。而累计 BP 每次更新权重和阈值是根据 $$\sum_{k=1}^{m} E_{k}$$ 为 cost function 计算的。

	5.6 试设计一个 BP 改进算法，能通过动态调整学习率显著提升收敛速度。编程实现该算法，并选择两个 UCI 数据集与标准 BP 算法进行试验比较。

    5.7 根据式（5.18）和 （5.19），试构造一个能解决亦或问题的单层 RBF 神经网络。
    中心是 (0, 0) 和 (1,1)
    beta 是个足够大的正数
    w_1 = 0, w_2 = 0

	5.8 从网络上下载或者自己实现 SOM 网络，并观察其在西瓜数据集 $3.0\alpha$ 上产生的结果。
	[网上给出的 SOM 模型](https://codesachin.wordpress.com/2015/11/28/self-organizing-maps-with-googles-tensorflow/)
    
    
	5.9 试推导用于 Elman 网络的 BP 算法。
	
    5.10 从网上或者自己实现一个卷积神经网络，并在手写字符识别数据 MNIST 上进行试验测试。
    [网上给出的卷积神经网络]()
