# 集成学习

### Intuition(主观意图)

问题困难时，我们会找朋友沟通商量解决。集成学习与这种情形类似： 有多个分类器，如何把多个分类器组合成一个分类效果比任何单个分类器都好的分类器。

举两个极端的例子：
如果有两个分类器A, B。如果 A 和 B 对所有样本点的预测完全一致（也就是说两个模型完全想听），那么组不组合没有什么意义。
如果有两个分类器 A, B。对于每一个样本点的预测，A 都比 B 准。那么组合的效果不会优于分类器A。

所以集成学习的目标是： 如何从多个模型中组合出一个比任何单个模型都好的模型。比如下面的例子。





### 常用算法

#### Boosting
    AdaBoost

$$H(\vec{x}) = sign(\sum_{t=1}^{T}\alpha_t h_{t}(\vec{x})$$


Cost Function

$$l_{exp}(H|D) = E_{x~D}[e^{-f(\vec{x}) H(\vec{x})}]$$

#### Bagging

#### 随机森林



### Questions

8.1 略
8.2 对于任意 l(我们用 l 替代 exp 函数)， 类似于 8.6 式得出：

$$\frac{\partial l_{exp}(H|D)}{\partial H(\vec{x})} = \frac{\partial l}{\partial H}|_{H(\vec{x})} P(f(\vec{x})=1|\vec{x}) - \frac{\partial l}{\partial H}|_{-H(\vec{x})} P(f(\vec{x})=-1|\vec{x})$$

要是上面的式子等于 0， 那么当 $P(f(\vec{x})=1|\vec{x}) > P(f(\vec{x})=-1|\vec{x})$ 时候，推出 $\frac{\partial l}{\partial H}|_{H(\vec{x})} < \frac{\partial l}{\partial H}|_{-H(\vec{x})}$ 由 $l$ 是单调减的，推出 $H(\vec{x}) > -H(\vec{x})$ 推出 $H(\vec{x}) > 0$ 推出 $sign(H(\vec{x})) = 1$, 同理当 $P(f(\vec{x})=1|\vec{x}) < P(f(\vec{x})=-1|\vec{x})$ 推出 $sign(H(\vec{x})) = -1$ 由此推出一致性。

8.3 从网上下载或自己编程实现 AdaBoost, 以不剪枝决策树为基学习器，在西瓜数据集 $3.0\alpha$ 上训练一个 AdaBoost 集成，并与图 8.4 进行比较。
8.4 GradientBoosting, Boosting： Boosting 通过重采样或者重新赋权重来集成模型。Gradient 通过更新损失函数，来集成模型。
8.5 试编程实现 Bagging, 以决策树桩为基学习器，在西瓜数据集3.0$\alpha$ 上训练一个 Bagging 集成，并与图 8.6 比较。
8.6 朴素贝叶斯对于不同样本训练出的模型差异太小，所以Bagging 通常难以提升朴素贝叶斯分类器的性能。
8.7 因为随机森林每次都在一个 features 的子空间中训练模型。特征少，所以计算量少。另外由于是在 featues 的子空间上训练，所以训练出来的模型差异比较大，利于 Bagging 去集成。
8.8 MultiBoosting 算法以 AdaBoost 模型做为 Bagging 模型的基学习器，AdaBoost 多样性高，但准确性低。Iterative Boosting 算法以 Bagging 学习器作为 AdaBoost 的基学习器，Bagging 的准确度高，但是多样性低。理想的是 基学习器应该是准确度高且多样性高。
8.9 这个问题太开放。比如我看以画个集合图，集合的交集是两个学习器预测相同的结果。主要把结果画出来重叠的面积越大，表示两个学习器的多样性越大。
8.10 明显可以用 bagging 算法。