# 神经网络


### Intuition

可能是对于人神经系统的研究，引出了神经网络模型。神经网络中的模型有很多概念的名字提起来很像神经系统的概念。


- 神经元: 经典的神经网络，每个神经元就是一个 logistic Classifier。不过可以使用不同的激励函数。

- 神经网络的表达能力非常强（只要神经元足够多，可以趋近任何连续函数）。

- BP 反向传播算法。求导的链式法则，sigmoid 函数的性质，使得对于神经网络的目标函数的求导变得很容易计算。BP 反向传播算法是根据梯度下降法设计的，算法的核心，就是对连接权和阈值求导。

$$f^{'}(x) = f(x) (1 - f(x)); sigmoid 函数良好的特性$$

![BP 算法](BP.png)

	神经网络的目标函数(Cost Function)可以是

    $$(f(x_i) - \vec{y_i})^T (f(x_i) - \vec{y_i}) $$

    也可以是累计误差

    $$\sum (f(x_i) - \vec{y_i})^T (f(x_i) - \vec{y_i}) $$

    上图给出的算法，针对的是没有累计的误差。

    最后，关于如何确定隐层神经元的个数，当前没有固定的方法可寻。很多人稍微改动下神经网络的结构，就能发篇文章。感觉这文章发地很轻松啊。

- 过拟合

    复杂模型都容易导致过拟合，神经网络也不例外。为了避免过拟合，可以预留 Validate DataSet, 训练模型时，每步迭代，都观察模型在 Validate DataSet 上的精度有没有下降，如果有下降，就停止迭代。

- 其他神经网络：RBF 网络，ART 网络，SOM 网络， Elem 网络， Boltzman 机。有时间再整这个吧？

- 深度学习：
	
    只是简单了解，最深的体会是，深度学习能训练训练特征。比如有人用深度学习训练模型，判断是不是动物，而当前要使用深度学习训练模型，判断是不是人。我们可以用判断是不是动物的模型训练好的特征做训练。这种方法在图像相关的内容中用的非常多，因为图像相关的内容有很多训练好的模型，直接在训练好的模型上做 Tuning 明显要方便很多。
    
    深度学习的挑战在于，如何高效的训练（求解连接权和阈值）。RNN 使用共享连接权的策略，减小参数个数，提高训练的效率。具体深度学习的内容，了解不多，还需要继续学习。


### Qustions

    5.1 试述将线性函数 $f(x) = \vec{w}^{T} \vec{x}$ 用作神经元激励函数的缺陷。
        如果用线性函数作为激励函数，那么最后预测函数也是个线性函数，神经网络也就不再具有表达复杂非线性的能力，比如对于线性不可分问题，将无法给出好的分类模型。
    
    5.2 试述使用图 5.2(b) 激活函数的神经元与对率回归的联系。
        每个神经元都是一个对率回归模型，神经网络模型成为对率回归函数的复合。
        
    5.3 对于图 5.7 中的 $v_{ih}$, 试推导出 BP 算法中的更新公式 (5.13).
    
$$\alpha_{h} = \sum_{i=1}^{d} {v_{ih} x_i} \Rightarrow \frac {\partial \alpha_{h}}{\partial {v_{ih}}} = x_i$$

$$\frac{\partial {b_h}}{\partial {v_{ih}}} = \frac {\partial {b_h}}{\partial {\alpha_{h}}} \frac {\partial \alpha_{h}}{\partial {v_{ih}}} \land \frac {\partial \alpha_{h}}{\partial {v_{ih}}} = x_i  \Rightarrow \frac{\partial {b_h}}{\partial {v_{ih}}} = \frac {\partial {b_h}}{\partial {\alpha_{h}}} x_i$$

$$\frac {\partial {E_{k}}} {\partial {v_{ih}}} = \frac {\partial {E_{k}}}{\partial {b_{h}}} \frac{\partial {b_h}}{\partial {v_{ih}}} \land \frac{\partial {b_h}}{\partial {v_{ih}}} = \frac {\partial {b_h}}{\partial {\alpha_{h}}} x_i \Rightarrow \frac {\partial {E_{k}}} {\partial {v_{ih}}} = \frac {\partial {E_{k}}}{\partial {b_{h}}} \frac {\partial {b_h}}{\partial {\alpha_{h}}} x_i$$

$$\frac {\partial {E_{k}}} {\partial {v_{ih}}} = \frac {\partial {E_{k}}}{\partial {b_{h}}} \frac {\partial {b_h}}{\partial {\alpha_{h}}} x_i \land e_h = \frac {\partial {E_{k}}}{\partial {b_{h}}} \frac {\partial {b_h}}{\partial {\alpha_{h}}} \Rightarrow \frac {\partial {E_{k}}} {\partial {v_{ih}}} = e_h x_i$$

$$\Delta v_{ih} = -\eta \frac {\partial {E_{k}}} {\partial {v_{ih}}} \land \frac {\partial {E_{k}}} {\partial {v_{ih}}} = e_h x_i \Rightarrow \Delta v_{ih} = - \eta e_h x_i$$
        
    5.4 试述式（5.6）中学习率的取值对于神经网络的训练的影响。
        学习率小，收敛太慢。学习率大，结果可能会震荡不收敛。
        
    5.5 试编程实现标准 BP 算法和累积 BP 算法，在西瓜数据集 3.0 上分别用这两个算法训练一个单隐层网络，并进行比较。
    	标准BP 和 累计 BP 算法的不同在于： 标准 BP 每次更新权重和阈值，是以某一个样本的 $$E_k$$ 为 cost function 计算的。而累计 BP 每次更新权重和阈值是根据 $$\sum_{k=1}^{m} E_{k}$$ 为 cost function 计算的。

	5.6 试设计一个 BP 改进算法，能通过动态调整学习率显著提升收敛速度。编程实现该算法，并选择两个 UCI 数据集与标准 BP 算法进行试验比较。

    5.7 根据式（5.18）和 （5.19），试构造一个能解决亦或问题的单层 RBF 神经网络。
    中心是 (0, 0) 和 (1,1)
    beta 是个足够大的正数
    w_1 = 0, w_2 = 0

	5.8 从网络上下载或者自己实现 SOM 网络，并观察其在西瓜数据集 $3.0\alpha$ 上产生的结果。
	[网上给出的 SOM 模型](https://codesachin.wordpress.com/2015/11/28/self-organizing-maps-with-googles-tensorflow/)
    
    
	5.9 试推导用于 Elman 网络的 BP 算法。
	
    5.10 从网上或者自己实现一个卷积神经网络，并在手写字符识别数据 MNIST 上进行试验测试。
    [网上给出的卷积神经网络]()
